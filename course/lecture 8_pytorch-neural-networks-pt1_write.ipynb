{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Introduction to Pytorch & Neural Networks\n",
    "\n",
    "**Jonny, June 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe the difference between `numpy` and `torch` arrays (`np.array` vs. `torch.Tensor`)\n",
    "- Explain fundamental concepts of neural networks such as layers, nodes, activation functions, etc.\n",
    "- Create a simple neural network in PyTorch for regression or classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_regression, make_circles, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from utils.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch is a Python-based tool for scientific computing that provides several main features:\n",
    "    - `torch.Tensor`, an n-dimensional array similar to that of `numpy`, but which can run on GPUs\n",
    "    - Computational graphs for building neural networks\n",
    "    - Automatic differentiation for training neural networks (more on this next lecture)\n",
    "- You can install PyTorch from: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch's Tensor\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch a tensor is just like NumPy's `ndarray` that we have become so familiar with.\n",
    "- A key difference between PyTorch's `torch.Tensor` and numpy's `np.array` is that `torch.Tensor` was constructed to integrate with GPUs and PyTorch's computational graphs (more on that next lecture though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `ndarray` vs `tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating and working with tensors is much the same as with numpy `ndarrays`\n",
    "- You can create a tensor with `torch.tensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "tensor_2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "tensor_3 = torch.tensor(np.array([1, 2, 3]))\n",
    "\n",
    "for t in [tensor_1, tensor_2, tensor_3]:\n",
    "    print(f\"{t}, dtype: {t.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch also comes with most of the `numpy` functions we're already familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # random normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just like in NumPy we can look at the shape of a tensor with the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Tensors and Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different dtypes have different memory and computational implications (see the end of [Lecture 1](https://pages.github.ubc.ca/MDS-2020-21/DSCI_572_sup-learn-2_students/lectures/lecture1_floating-point.html#precision-vs-memory-vs-speed) or [Lecture 5 of DSCI 511](https://pages.github.ubc.ca/MDS-2020-21/DSCI_511_py-prog_students/lectures/lecture5-numpy-addendum.html) for more)\n",
    "- In Pytorch we'll be building networks that require thousands or even millions of floating point calculations!\n",
    "- In such cases, using a smaller dtype like `float32` can significantly speed up computations and reduce memory requirements\n",
    "- The default float dtype in pytorch `float32`, as opposed to numpy's `float64`\n",
    "- In fact some operations in Pytorch will even throw an error if you pass a high-memory `dtype`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But just like in numpy, you can always specify the particular dtype you want using the `dtype` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensors operate just like `ndarrays` and have a variety of familiar methods that can be called off them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # broadcasting betweean a 1 x 3 and 3 x 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once again, same as NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Numpy Bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes we might want to convert a tensor back to a NumPy array\n",
    "- We can do that using the `.numpy()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. GPU and CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU is a graphical processing unit (as opposed to a CPU: central processing unit)\n",
    "- GPUs were originally developed for gaming, they are very fast at performing operations on large amounts of data by performing them in parallel (think about updating the value of all pixels on a screen very quickly as a player moves around in a game)\n",
    "- More recently, GPUs have been adapted for more general purpose programming\n",
    "- Neural networks can typically be broken into smaller computations that can be performed in parallel on a GPU\n",
    "- PyTorch is tightly integrated with CUDA - a software layer that facilitates interactions with a GPU (if you have one)\n",
    "- You can check if you have GPU capability using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()  # my MacBook Pro does not have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When training on a machine that has a GPU, you need to tell PyTorch you want to use it\n",
    "- You'll see the following at the top of most PyTorch code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can then use the `device` argument when creating tensors to specify whether you wish to use a CPU or GPU\n",
    "- Or if you want to move a tensor between the CPU and GPU, you can use the `.to()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 2, 2, device=device)\n",
    "print(X.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.to('cuda')  # this would give an error for me so I'm commenting out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll revisit GPUs later in the course when we are working with bigger datasets and more complex networks\n",
    "- For now, we can work on the CPU just fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Basics\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You've already learned about several machine learning algorithms (kNN, Random Forest, SVM, etc.)\n",
    "- Neural networks are simply another algorithm and actually one of the simplest in my opinion!\n",
    "- As we'll see, a neural network is just a sequence of linear and non-linear transformations\n",
    "- Often you see something like this when learning about/using neural networks:\n",
    "\n",
    "![](img/nn-6.png)\n",
    "\n",
    "- So what on Earth does that all mean? Well we are going to build up some intuition one step at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Simple Linear Regression with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a simple regression dataset with 500 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We know how to fit a simple linear regression to this data using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are the parameters of that fitted line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As an equation, that looks like this:\n",
    "\n",
    "$$\\hat{y}=-0.77 + 45.50X$$\n",
    "\n",
    "- Or in matrix form:\n",
    "\n",
    "$$\\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ \\vdots \\\\ \\hat{y_n} \\end{bmatrix}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} -0.77 \\\\ 45.55 \\end{bmatrix}$$\n",
    "\n",
    "- Or in graph form I'll represent it like this: \n",
    "\n",
    "![](img/nn-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Linear Regression with a Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So let's implement the above in PyTorch to start gaining an intuition about neural networks!\n",
    "- Every neural network model you build in PyTorch will inherit from `torch.nn.Module`\n",
    "- Remember [class inheritance from DSCI 511](https://www.tomasbeuzen.com/python-programming-for-data-science/lectures/lecture3-tests-classes.html#inheritance-subclasses)? Inheritance allows us to inherit commonly needed functionality without having to write code ourselves!\n",
    "\n",
    ">We'll explore `torch.nn.Module` more in the lab but if it helps, think about sklearn models: they all inherit common methods like `.fit()`, `.predict()`, `.score()`, etc. When creating a neural network, we define our own architecture but still want common functionality which we inherit from `torch.nn.Module`.\n",
    "\n",
    "- Let's create a model called `linearRegression` and then I'll talk you through the syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the above:\n",
    "\n",
    "```python\n",
    "class linearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__() \n",
    "```\n",
    "\n",
    "^ Here we're creating a class called `linearRegression` and inheriting the methods and attributes of `nn.Module` (hint: try typing `help(linearRegression)` to see all the things we inheritied from `nn.Module`).\n",
    "\n",
    "```python\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "```\n",
    "\n",
    "^ Here we're defining a \"Linear\" layer, which just means `wX + b`, i.e., the weights of the network, multiplied by the input features plus the bias.\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "```\n",
    "\n",
    "^ PyTorch networks created with `nn.Module` must have a `forward()` method. It accepts the input data `x` and passes it through the defined operations. In this case, we are passing `x` into our linear layer and getting an output `out`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After defining the model class, we can create an instance of that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/nn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can check out our model using `print()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or the more useful `summary()` (which we imported at the top of this notebook with `from torchsummary import summary`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice how we have two parameters? We have one for the weight (`w1`) and one for the bias (`w0`)\n",
    "- These were initialized randomly by PyTorch when we created our model. They can be accessed with `model.state_dict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay, before we move on, our `x` and `y` data are currently numpy arrays but they need to be PyTorch tensors\n",
    "- Let's convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a working model right now and could tell it to give us some output with this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our prediction is pretty bad because our model is not trained/fitted yet!\n",
    "- As we learned in the past few lectures, to fit our model we need:\n",
    "    1. **a loss function** (called \"criterion\" in PyTorch) to tell us how good/bad our predictions are - we'll use mean squared error, `torch.nn.MSELoss()`\n",
    "    2. **an optimization algorithm** to help optimise model parameters - we'll use GD, `torch.optim.SGD()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we train I'm going to create a \"data loader\" to help batch my data\n",
    "- We'll talk more about these next lecture and in lab but they are just generators that yield data to us on request ([remember generators from 511?](https://pages.github.ubc.ca/MDS-2020-21/DSCI_511_py-prog_students/lectures/lecture2-loops-functions.html#generators))\n",
    "- We'll use a `BATCH_SIZE = 50` (which should give us 10 batches because we have 500 data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We should have 10 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can look at a batch using this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With those, let's train our simple network for 5 epochs of SGD!\n",
    "> I'll explain all the code here next lecture but scan throught it, it's not too hard to see what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now our model has been trained, our parameters should be different than before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing to our sklearn model, we get the same answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We got pretty close! We could do better by changing the number of epochs or the learning rate\n",
    "- So here is our simple network once again:\n",
    "\n",
    "![](img/nn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By the way, check out what happens if we run `trainer()` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model continues where we left off!\n",
    "- This may or may not be what you want. We can start from scratch by re-making our `model` and `optimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Multiple Linear Regression with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay, let's do a multiple linear regression now with 3 features\n",
    "- So our network will look like this:\n",
    "\n",
    "![](img/nn-3.png)\n",
    "\n",
    "- Let's go ahead and create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = make_regression(n_samples=500, n_features=3, random_state=0, noise=10.0)\n",
    "X_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32)\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And let's create the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linearRegression(input_size=3, output_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We should now have 4 parameters (3 weights and 1 bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks good to me! Let's train the model and then compare it to sklearn's `LinearRegression()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LinearRegression().fit(X, y)\n",
    "pd.DataFrame({\"w0\": [sk_model.intercept_, model.state_dict()['linear.bias'].item()],\n",
    "              \"w1\": [sk_model.coef_[0], model.state_dict()['linear.weight'][0, 0].item()],\n",
    "              \"w2\": [sk_model.coef_[1], model.state_dict()['linear.weight'][0, 1].item()],\n",
    "              \"w3\": [sk_model.coef_[2], model.state_dict()['linear.weight'][0, 2].item()]},\n",
    "             index=['sklearn', 'pytorch']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Non-linear Regression with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay so we can make a simple network to imitate simple and multiple *linear* regression\n",
    "- You're probably thinking, so what? But we're getting to the good stuff I promise!\n",
    "- For example, what happens when we have more complicated datasets like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "\n",
    "# Create dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is obviously non-linear, and we need to introduce some **non-linearities** into our network\n",
    "- These non-linearities are what make neural networks so powerful and they are called **\"activation functions\"**\n",
    "- We are going to create a new model class that includes a non-linearity - a sigmoid function:\n",
    "\n",
    "$$S(X)=\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll talk more about activation functions later, but note how the sigmoid function non-linearly maps `x` to a value between 0 and 1\n",
    "- Okay, so let's create the following network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/nn-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All this means is that the value of each node in the hidden layer will be transformed by the \"activation function\", thus introducing non-linear elements to our model!\n",
    "- There's two main ways of creating the above model in PyTorch, I'll show you both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note how our `forward()` method now passes `x` through the `nn.Sigmoid()` function after the hidden layer\n",
    "- The above method is very clear and flexible, but I prefer using `nn.Sequential()` to combine my layers together in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's make an instance of our new class and confirm it has 10 parameters (6 weights + 4 biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay, let's train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a look at those non-linear predictions! Cool!\n",
    "- Our model is not great and we could make it better soon by adjusting the learning rate, the number of nodes, and the number of epochs\n",
    "- But I really want you to see how **each of our hidden nodes is \"engineering a non-linear feature\"** to be used for the predictions, check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You've probably heard the magic term \"deep learning\" and you're about to find out what it means!\n",
    "- Really, it's just a neural network with more than 1 hidden layer! Easy!\n",
    "- I like to think of each layer as a \"feature engineerer\", it is trying to extract the maximum amount of information from the layer before it\n",
    "- There are arguments of \"deep network\" (many layers) vs \"wide network\" (many nodes), but for big datasets, \"deep networks\" have been shown to be very effective in practice\n",
    "- Let's create a \"deep\" network of 2 layers:\n",
    "\n",
    "![](img/nn-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But just because the network is deeper, doesn't always mean it's better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we learned above, activation functions are what allow us to model complex, non-linear functions\n",
    "- There are **many** different activations functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation functions should be non-linear and tend to be monotonic and continuously differentiable (smooth)\n",
    "- But as you can see with the ReLU function above, that's not always the case!\n",
    "- I wanted to point this out because it highlights how much of an art deep learning really is.\n",
    "- Here's a great quote from [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) (famous for his work in AI and deep learning) on his group experimenting with ReLU:\n",
    "\n",
    ">\"...one of the biggest mistakes I made was to think, like everyone else in the 90s, that you needed smooth non-linearities in order for backpropagation to work. because I thought that if we had something like rectifying nonlinearities, where you have a flat part, that it would be really hard to train, because the derivative would be zero in so many places. And when we started experimenting with ReLU, with deep nets around 2010, I was obsessed with the idea that, we should be careful about whether neurons won't saturate too much on the zero part. **But in the end, it turned out that, actually, the ReLU was working a lot better than the sigmoids and tanh, and that was a big surprise**...it turned out to work better, whereas I thought it would be harder to train!\"\n",
    "\n",
    "- Anyway, TL;DR **ReLU is probably the most popular these days**, but you can treat activation functions as hyperparams (to be optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Classification\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will actually be the easiest part of the lecture\n",
    "- Up until now, we've been looking at developing networks for regression tasks, but what if we want to do binary classification?\n",
    "- Well, what did we do in Logistic Regression? We just passed the output of a regression into the Sigmoid Function to get a value between 0 and 1 (a probability of an observation belonging to the positive class) - we'll do the same thing here!\n",
    "- Let's create a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create this network to model that dataset:\n",
    "\n",
    "![](img/nn-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm going to start using `ReLU` as our activation function(s) and `Adam` as our optimizer because these are what are currently, commonly used in practice.\n",
    "- We are doing classificatio now so we'll need to use log loss (binary cross entropy) as our loss function:\n",
    "\n",
    "$$f(w) = \\sum_{x,y \\in D} -y log(\\hat{y}) - (1-y)log(1-\\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch, binary cross entropy loss criterion is `torch.nn.BCELoss`\n",
    "- The formula expects a \"probability\" which is why we add a Sigmoid function to the end of out network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BUT WAIT!**\n",
    "- While we can do the above and then train with a `torch.nn.BCELoss` loss function, there's a better way!\n",
    "- We can omit the Sigmoid function and just use `torch.nn.BCEWithLogitsLoss` (which combines a Sigmoid layer and the BCELoss)\n",
    "- Why would we do this? It's numerically stable! (Did you do the log-sum-exp question in Lab 1? We use it here for stability!)\n",
    "- From the docs:\n",
    ">\"*This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.*\"\n",
    "- So actually, here's our model (no Sigmoid layer at the end because it's included in the loss function we'll use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To be clear, our model is just outputting some number between -∞ and +∞ (we aren't applying Sigmoid), so:\n",
    "    - To get the probabilities we would need to pass them through a Sigmoid;\n",
    "    - To get classes, we can apply some threshold (usually 0.5)\n",
    "- For example, we would expect the point (0,0) to have a high probability and the point (-1,-1) to have a low probability:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
